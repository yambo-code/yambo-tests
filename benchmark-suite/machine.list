/*
 * Copyright (C) 2001-2012 Quantum ESPRESSO group
 *
 * This file is distributed under the terms of the
 * GNU General Public License. See the file `License'
 * in the root directory of the present distribution,
 * or http://www.gnu.org/copyleft/gpl.txt .
 *
 */

*** Last update: April 5, 2016 (Andrea Ferretti) ***


# CINECA FERMI 
- Model            : IBM-BG/Q
- Architecture     : 10 BGQ Frame
- Nodes            : 10240
- Processors       : IBM PowerA2, 1.6 GHz
- RAM              : 1 Gbyte / core
- Internal Network : Network interface with 11 links ->5D Torus
- Operating System : Red Hat Enterprise Linux Server release 6.2 (Santiago) for the login node - dedicated Compute Node Kernel (CNK)
- Peak Performance : 2 PFlop/s (1.72 PFlops sustained - Linpack benchmark)
- Website          : http://www.hpc.cineca.it/hardware/fermi


# CINECA GALILEO 
- Model            : IBM NeXtScale
- Architecture     : Linux Infiniband Cluster
- Nodes            : 516
- Processors       : 2 8-cores Intel Haswell 2.40 GHz per node
- Cores            : 16 cores/node, 8256 cores in total
- Accelerators     : 2 Intel Phi 7120p per node on 384 nodes (768 in total); 
                     2 NVIDIA K80 per node on 40 nodes (80 in total, 20 available for scientific research)
- RAM              : 128 GB/node, 8 GB/core
- Internal Network : Infiniband with 4x QDR switches
- Operating System : 
- Peak Performance : 1.1 PFlop/s (peak), 0.68 PFlop/s (max)
- Website          : http://www.hpc.cineca.it/hardware/galileo


# TGCC-CEA CURIE #
- Model            : Bull system
- Architecture     : Linux Infiniband Cluster
- Nodes:
  - "Fat" nodesÂ (total: 360)
    - Processors  : 4 eight-cores CPU Nehalem-EX @ 2.27 GHz (11520 cores total)
    - RAM         : 128 Gb/node (4 Gb/core)
  - "Thin" nodes (total : 5040)
    - Processors  : 2 eight-cores CPU Sandy Bridge @ 2.7 GHz (AVX) (80640 cores total)
    - RAM         : 64 Gb/node (4 Gb/core) 
  - "Hybrid" nodes (total: 144)
    - Processors  : 2 four-cores CPU Westmere-EP @ 2.67 GHz
    - RAM         : 24 Gb/node (3 Gb/core)
    - Accelerator : 2 GPU Nvidia M2090
- Internal Network : InfiniBand QDR Full Fat Tree network
- Operating System : Linux
- Peak Performance : 2 PFlop/s (aggregated)
- Website          : http://www-hpc.cea.fr/en/complexe/tgcc-curie.htm


# CINECA SP6 
- Model            : IBM-Power6
- Architecture     : 12 Power6 Frame
- Nodes            : 168
- Processors       : 5376 cores, IBM Power6, 4.7 GHz
- RAM              : 4 Gbyte / core
- Internal Network : InfiniBand DDR
- Operating System : AIX6
- Peak Performance : 100 TFlop/s (80 TFlops sustained - Linpack benchmark)
- Website          : http://www.hpc.cineca.it/hardware/ibm-plx


# CNR-NANO HYDRA
- Model            : 
- Architecture     : Linux Infiniband Cluster
- Nodes            : 10 (6c proc) + 30 (8c proc)
- Processors       : 2 6-cores Intel Xeon CPU   X5650    @ 2.67GHz
                     2 8-cores Intel Xeon CPU E5-2650 0  @ 2.00GHz
                     2 8-cores Intel Xeon CPU E5-2650 v2 @ 2.60GHz
                     2 8-cores Intel Xeon CPU E5-2640 v3 @ 2.60GHz
- Cores            : 12-16 cores/node, 480 + 120 nodes total
- Accelerators     : none
- RAM              : 64-128 GB/node, 4-8 GB/core
- Internal Network : Infiniband QDR Mellanox
- Operating System : 
- Peak Performance : 
- Website          : http://www.labcsai.unimore.it/site/home/sistemi-hpc/hydra.hpc.unimo.it.html

